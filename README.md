# Berkeley Deep Drive-X (eXplanation) Dataset

We focus on generating textual descriptions and explanations, such as the pair:<br />
*“Vehicle slows down”* (description) and *“Because it is approaching an intersection and the light is red”* (explanation)

<center><img src="/img/overview.png" width="90%" height="90%"></center>

### Preview of the Annotations.
<iframe width="640" height="360" src="https://www.youtube.com/embed/6Az2cNU7gUw" frameborder="0" gesture="media" allowfullscreen=""></iframe>

[![Video Label](/img/bdd-x.png)](https://youtu.be/kbft0HEWdpk)

### Statistics.
Over 77 hours of driving with time-stamped human annotations for action descriptions and justifications.
<center><img src="/img/statistics.png" width="20%" height="20%"></center>

### Citation.
If you find this dataset useful, please cite this paper (and refer the data as Berkeley DeepDrive eXplanation or BDD-X dataset):
```
@article{kim2018textual,
  title={Textual Explanations for Self-Driving Vehicles},
  author={Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep},
  journal={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2018}
}
```

### How we collect?
Our explanation dataset is built on top of Berkeley Deep Drive dataset (<https://bdd-data.berkeley.edu/>) collected from dashboard cameras in human driven vehicles. Annotators view the video dataset, compose descriptions of the vehicle’s activity and explanations for the actions that the vehicle driver performed.

<center><img src="/img/interface.png" width="70%" height="70%"></center>




